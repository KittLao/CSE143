# -*- coding: utf-8 -*-
"""Copy of 16_nlp_with_rnns_and_attention.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1MZbsbVRWhsI6jt50GtBQGlJLrIbraF_t

**Chapter 16 – Natural Language Processing with RNNs and Attention**

_This notebook contains all the sample code in chapter 16._

<table align="left">
  <td>
    <a target="_blank" href="https://colab.research.google.com/github/jflanigan/handson-ml2/blob/master/16_nlp_with_rnns_and_attention.ipynb"><img src="https://www.tensorflow.org/images/colab_logo_32px.png" />Run in Google Colab</a>
  </td>
</table>

# Setup

First, let's import a few common modules, ensure MatplotLib plots figures inline and prepare a function to save the figures. We also check that Python 3.5 or later is installed (although Python 2.x may work, it is deprecated so we strongly recommend you use Python 3 instead), as well as Scikit-Learn ≥0.20 and TensorFlow ≥2.0.
"""

# Commented out IPython magic to ensure Python compatibility.
# Python ≥3.5 is required
import sys
assert sys.version_info >= (3, 5)

# Scikit-Learn ≥0.20 is required
import sklearn
assert sklearn.__version__ >= "0.20"

try:
    # %tensorflow_version only exists in Colab.
#     %tensorflow_version 2.x
    !pip install -q -U tensorflow-addons
    IS_COLAB = True
except Exception:
    IS_COLAB = False

# TensorFlow ≥2.0 is required
import tensorflow as tf
from tensorflow import keras
assert tf.__version__ >= "2.0"

if not tf.test.is_gpu_available():
    print("No GPU was detected. LSTMs and CNNs can be very slow without a GPU.")
    if IS_COLAB:
        print("Go to Runtime > Change runtime and select a GPU hardware accelerator.")

# Common imports
import numpy as np
import os

# to make this notebook's output stable across runs
np.random.seed(42)
tf.random.set_seed(42)

# To plot pretty figures
# %matplotlib inline
import matplotlib as mpl
import matplotlib.pyplot as plt
mpl.rc('axes', labelsize=14)
mpl.rc('xtick', labelsize=12)
mpl.rc('ytick', labelsize=12)

# Where to save the figures
PROJECT_ROOT_DIR = "."
CHAPTER_ID = "nlp"
IMAGES_PATH = os.path.join(PROJECT_ROOT_DIR, "images", CHAPTER_ID)
os.makedirs(IMAGES_PATH, exist_ok=True)

def save_fig(fig_id, tight_layout=True, fig_extension="png", resolution=300):
    path = os.path.join(IMAGES_PATH, fig_id + "." + fig_extension)
    print("Saving figure", fig_id)
    if tight_layout:
        plt.tight_layout()
    plt.savefig(path, format=fig_extension, dpi=resolution)

"""# Sentiment Analysis"""

tf.random.set_seed(42)

"""You can load the IMDB dataset easily:"""

"""
Load the IMDB data sets. The data set consists of the training, testing,
and validation. Each data set is an array of sentences, where each
sentence is an array of number. The numbers map to a unique word.
"""
(X_train, y_test), (X_valid, y_test) = keras.datasets.imdb.load_data()

"""
word_index is the dictionary that maps the word to it's unuique
number.
id_to_word is the dictionary that maps a unique id to a unique
word.
The first 3 id's are special words.
The last line is the actual first 10 words of an instance.
"""
word_index = keras.datasets.imdb.get_word_index()
id_to_word = {id_ + 3: word for word, id_ in word_index.items()}
for id_, token in enumerate(("<pad>", "<sos>", "<unk>")):
    id_to_word[id_] = token
" ".join([id_to_word[id_] for id_ in X_train[0][:10]])

import tensorflow_datasets as tfds

#datasets, info = tfds.load("imdb_reviews", as_supervised=True, with_info=True)
(train_data, validation_data, test_data), info = tfds.load(
    name="imdb_reviews", 
    split=('train[:85%]', 'train[15%:]', 'test'),
    as_supervised=True, with_info=True)

datasets = {'train' : train_data, 'test' : test_data, 'validation' : validation_data}

type(train_data)

train_size = info.splits["train"].num_examples # 25000
test_size = info.splits["test"].num_examples # 25000
unsupervised_size = info.splits["unsupervised"].num_examples # 50000

"""
Examsples of train data
"""
for X_batch, y_batch in datasets["train"].batch(2).take(1):
    for review, label in zip(X_batch.numpy(), y_batch.numpy()):
        print("Review:", review.decode("utf-8")[:200], "...")
        print("Label:", label, "= Positive" if label else "= Negative")
        print()

"""
Examsples of validation data
"""
for X_batch_v, y_batch_v in datasets["validation"].batch(2).take(1):
    for review_v, label_v in zip(X_batch_v.numpy(), y_batch_v.numpy()):
        print("Review:", review_v.decode("utf-8")[:200], "...")
        print("Label:", label_v, "= Positive" if label_v else "= Negative")
        print()

"""
Examsples of validation data
"""
for X_batch_t, y_batch_t in datasets["test"].batch(2).take(1):
    for review_t, label_t in zip(X_batch_t.numpy(), y_batch_t.numpy()):
        print("Review:", review_t.decode("utf-8")[:200], "...")
        print("Label:", label_t, "= Positive" if label_t else "= Negative")
        print()

"""
For each review, keep only the first 300 words because you can generally
tell if a review is positive or not, depending on the first few sentences.
Use regular expression to replace all the <br/> tags with spaces, and to
replace any characters other than letters adn quotes with spaces. Finally,
splits the reviews by spaces.
"""
def preprocess(X_batch, y_batch):
    X_batch = tf.strings.substr(X_batch, 0, 300)
    X_batch = tf.strings.regex_replace(X_batch, rb"<br\s*/?>", b" ")
    X_batch = tf.strings.regex_replace(X_batch, b"[^a-zA-Z']", b" ")
    X_batch = tf.strings.split(X_batch)
    return X_batch.to_tensor(default_value=b"<pad>"), y_batch

preprocess(X_batch, y_batch)

preprocess(X_batch_v, y_batch_v)

preprocess(X_batch_t, y_batch_t)

"""
Construct a dictionary of vocabulary for the batches
using the train data.
"""
from collections import Counter

vocabulary = Counter() # 41624.
# use most_common()
for X_batch, y_batch in datasets["train"].batch(32).map(preprocess):
    for review in X_batch:
        vocabulary.update(list(review.numpy()))

"""
The model doesn't need to know every single word. Words
that appear less than 10000 times can be truncated.
"""
vocab_size = 10000
truncated_vocabulary = [
    word for word, count in vocabulary.most_common()[:vocab_size]]

"""
Create a word to id mapping. Test it using a string
"""
word_to_id = {word: index for index, word in enumerate(truncated_vocabulary)}
for word in b"This movie was faaaaaantastic".split():
    print(word_to_id.get(word) or vocab_size)

"""
Replace each word with it's ID.
table is a lookup table 1000 OOV buckets.
"""
words = tf.constant(truncated_vocabulary)
word_ids = tf.range(len(truncated_vocabulary), dtype=tf.int64)
vocab_init = tf.lookup.KeyValueTensorInitializer(words, word_ids)
num_oov_buckets = 1000
table = tf.lookup.StaticVocabularyTable(vocab_init, num_oov_buckets)

table.lookup(tf.constant([b"This movie was faaaaaantastic".split()]))

"""
Batch the reviews and convert them to short sequences
of words using preprocess. Do this for train, test,
and validation.
"""
def encode_words(X_batch, y_batch):
    return table.lookup(X_batch), y_batch

# use take()

"""
All these data used for entire assignment
"""
train_set = datasets["train"].repeat().batch(32).map(preprocess)
train_set = train_set.map(encode_words).prefetch(1)

test_set = datasets["test"].batch(32).map(preprocess)
test_set = test_set.map(encode_words).prefetch(1)

validation_set = datasets["validation"].batch(32).map(preprocess)
validation_set = validation_set.map(encode_words).prefetch(1)

# My code begins

def part1(num_epochs, train_batch_size_choice=32, activation_func='tanh', choice_optimizer='adam', drop_out_choice=0.0):
  embed_size = 128
  model = keras.models.Sequential([
      keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True),
      keras.layers.SimpleRNN(embed_size, activation=activation_func, use_bias=True, \
                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal', \
                            bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, 
                            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, 
                            recurrent_constraint=None, bias_constraint=None, dropout=drop_out_choice, 
                            recurrent_dropout=0.0, return_sequences=False, return_state=False, 
                            go_backwards=False, stateful=False, unroll=False),
      keras.layers.Dense(1, activation="sigmoid")
  ])
  model.compile(loss="binary_crossentropy", optimizer=choice_optimizer, metrics=["accuracy"])
  history = model.fit(train_set, steps_per_epoch=train_size // train_batch_size_choice, epochs=num_epochs, validation_data=validation_set)
  test = model.evaluate(test_set)

# part1(10, 32, 'tanh', 'adam', 0.2)
# part1(5, 32, 'tanh', 'adam', 0.2)
# part1(20, 32, 'tanh', 'adam', 0.2)

# part1(10, 32, 'tanh', 'adam', 0.2)
# part1(10, 16, 'tanh', 'adam', 0.2)
# part1(10, 64, 'tanh', 'adam', 0.2)

# part1(10, 32, "tanh", 'adam', 0.2)
# part1(10, 32, "sigmoid", 'adam', 0.2)
# part1(10, 32, "relu", 'adam', 0.2)

# part1(10, 32, 'tanh', 'adam', 0.2)
# part1(10, 32, 'tanh', 'sgd', 0.2)
# part1(10, 32, 'tanh', 'rmsprop', 0.2)

# part1(10, 32, 'tanh', 'adam', 0.0)
# part1(10, 32, 'tanh', 'adam', 0.2)
# part1(10, 32, 'tanh', 'adam', 0.4)

def part2(num_epochs, train_batch_size_choice=32, activation_func='tanh', choice_optimizer='adam', drop_out_choice=0.0):
  embed_size = 128
  model = keras.models.Sequential([
      keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size, mask_zero=True),
      keras.layers.LSTM(embed_size, activation=activation_func, use_bias=True,
                            kernel_initializer='glorot_uniform', recurrent_initializer='orthogonal',
                            bias_initializer='zeros', kernel_regularizer=None, recurrent_regularizer=None, 
                            bias_regularizer=None, activity_regularizer=None, kernel_constraint=None, 
                            recurrent_constraint=None, bias_constraint=None, dropout=drop_out_choice, 
                            recurrent_dropout=0.0, return_sequences=False, return_state=False, 
                            go_backwards=False, stateful=False, unroll=False),
      keras.layers.Dense(1, activation="sigmoid")
  ])
  model.compile(loss="binary_crossentropy", optimizer=choice_optimizer, metrics=["accuracy"])
  history = model.fit(train_set, steps_per_epoch=train_size // train_batch_size_choice, epochs=num_epochs, validation_data=validation_set)
  test = model.evaluate(test_set)

# part2(10, 32, 'tanh', 'adam', 0.0)
# part2(5, 32, 'tanh', 'adam', 0.0)
# part2(20, 32, 'tanh', 'adam', 0.0)

# part2(10, 32, 'tanh', 'adam', 0.2)
# part2(10, 16, 'tanh', 'adam', 0.2)
# part2(10, 64, 'tanh', 'adam', 0.2)

# part2(10, 32, "tanh", 'adam', 0.2)
# part2(10, 32, "sigmoid", 'adam', 0.2)
# part2(10, 32, "relu", 'adam', 0.2)

# part2(10, 32, 'tanh', 'adam', 0.2)
# part2(10, 32, 'tanh', 'sgd', 0.2)
# part2(10, 32, 'tanh', 'rmsprop', 0.2)

# part2(10, 32, 'tanh', 'adam', 0.0)
# part2(10, 32, 'tanh', 'adam', 0.2)
# part2(10, 32, 'tanh', 'adam', 0.4)

# Commented out IPython magic to ensure Python compatibility.
"""
Part 3
"""
# % tensorflow_version 2.x
from tensorflow.keras.preprocessing.text import Tokenizer
from tensorflow.keras.preprocessing.sequence import pad_sequences
from tensorflow.keras.models import Sequential
from tensorflow.keras.layers import Dense, Flatten, LSTM, Conv1D, MaxPooling1D, Dropout, Activation
from tensorflow.keras.layers import Embedding
from tensorflow.keras.preprocessing.sequence import pad_sequences
import numpy as np

from tensorflow.keras.datasets import imdb

embeddings_index = dict()
f = open('glove.6B.100d.txt', encoding='utf8')

for line in f:
    values = line.split()
    word = values[0]
    coefs = np.asarray(values[1:], dtype='float32')
    embeddings_index[word] = coefs
f.close()

# To create the embedding matrix:

vocabulary_size = 10000

word2id = imdb.get_word_index()   # dictionary from words to integers (the id of the word in the vocab)
id2word = {i: word for word, i in word2id.items()}
# Embedding matrix holds the vector representation for the words.
embedding_matrix = np.zeros((vocabulary_size, 100)) # 50000
for word, index in word2id.items():
   if index > vocabulary_size - 1:
       continue
   else:
       embedding_vector = embeddings_index.get(word)
       if embedding_vector is not None:
           embedding_matrix[index] = embedding_vector

# for i in range(0, 3):
#    print("The glove embedding for '{}' is {} ".format(list(word2id.keys())[i], embedding_matrix[i]))

(X_train_full, y_train_full), (X_test, y_test) = imdb.load_data(num_words = vocabulary_size)

train_size = len(X_train)
X_train_full = pad_sequences(X_train_full, maxlen=1000)
X_train, X_valid = X_train_full[5000:], X_train_full[:5000]
y_train, y_valid = y_train_full[5000:], y_train_full[:5000]

X_test = pad_sequences(X_test, maxlen=1000)

# activation_choice = 'tanh'
# drop_out_choice = 0.0
# optimizer_choice = 'adam'
# train_batch_size_choice = 32
# epochs_choice = 5

def part3(num_epochs, train_batch_size_choice, activation_choice, optimizer_choice, drop_out_choice):
  embed_size = 128
  model_glove = Sequential()
  model_glove.add(Embedding(vocabulary_size, 100, 
                  weights=[embedding_matrix], trainable=False))
  model_glove.add(LSTM(embed_size, activation=activation_choice, dropout=drop_out_choice))
  model_glove.add(Dense(1, activation="sigmoid"))
  model_glove.compile(loss='binary_crossentropy', optimizer=optimizer_choice, 
                  metrics=['accuracy'])
  model_glove.fit(X_train, y_train, steps_per_epoch=train_batch_size_choice
                  , epochs=num_epochs, validation_data=(X_valid, y_valid))
  model_glove.evaluate(X_test, y_test)
  return model_glove

# part3(10, 250, 'tanh', 'adam', 0.0)
# part3(5, 250, 'tanh', 'adam', 0.0)
# part3(20, 250, 'tanh', 'adam', 0.0)

# part3(10, 250, 'tanh', 'adam', 0.0)
# part3(10, 125, 'tanh', 'adam', 0.0)
# part3(10, 500, 'tanh', 'adam', 0.0)

# part3(10, 250, 'tanh', 'adam', 0.0)
# part3(10, 250, 'sigmoid', 'adam', 0.0)
# part3(10, 250, 'relu', 'adam', 0.0)

# part3(10, 250, 'tanh', 'adam', 0.0)
# part3(10, 250, 'tanh', 'sgd', 0.0)
# part3(10, 250, 'tanh', 'rmsprop', 0.0)

# part3(5, 250, 'tanh', 'adam', 0.0)
# part3(10, 250, 'tanh', 'adam', 0.2)
# part3(10, 250, 'tanh', 'adam', 0.4)

import math

def cosine_similarity(w1, w2):
  dotProd = lambda x, y: sum([x[i] * y[i] for i in range(len(x))])
  return dotProd(w1, w2) / (math.sqrt(dotProd(w1, w1)) * math.sqrt(dotProd(w2, w2)))

antonyms = [('happy', 'sad'), ('good', 'bad'), ('inferior', 'superior'),
            ('agree', 'disagree'), ('boy', 'girl'), ('true', 'false'),
            ('push', 'pull'), ('offense', 'defense'), ('wife', 'husband'),
            ('sit', 'stand')]


antonym_vecs = [[embedding_matrix[word2id[antonym[0]]], embedding_matrix[word2id[antonym[1]]]] for antonym in antonyms]

cosine_similarities = [cosine_similarity(antonym_vec[0], antonym_vec[1]) for antonym_vec in antonym_vecs]

for i in range(10):
  print(antonyms[i], ":", cosine_similarities[i])

w1 = "this film was the greatest film i have ever seen"
w2 = "this film was the worst film i have ever seen"

w1_ = [word2id[w] for w in w1.split()]
w2_ = [word2id[w] for w in w2.split()]

embed_size = 128
model_glove = Sequential()
model_glove.add(Embedding(vocabulary_size, 100, 
                  weights=[embedding_matrix], trainable=False))
model_glove.add(LSTM(embed_size, activation='tanh', dropout=0.0))
model_glove.add(Dense(1, activation="sigmoid"))
model_glove.compile(loss='binary_crossentropy', optimizer='adam', 
                  metrics=['accuracy'])
model_glove.fit(X_train, y_train, steps_per_epoch=250
                  , epochs=1, validation_data=(X_valid, y_valid))

model_glove.predict([w1_, w2_])

# My code ends

"""Or using manual masking:"""

K = keras.backend
embed_size = 128
inputs = keras.layers.Input(shape=[None])
mask = keras.layers.Lambda(lambda inputs: K.not_equal(inputs, 0))(inputs)
z = keras.layers.Embedding(vocab_size + num_oov_buckets, embed_size)(inputs)
z = keras.layers.GRU(128, return_sequences=True)(z, mask=mask)
z = keras.layers.GRU(128)(z, mask=mask)
outputs = keras.layers.Dense(1, activation="sigmoid")(z)
model = keras.models.Model(inputs=[inputs], outputs=[outputs])
model.compile(loss="binary_crossentropy", optimizer="adam", metrics=["accuracy"])
history = model.fit(train_set, steps_per_epoch=train_size // 32, epochs=5)

"""## Reusing Pretrained Embeddings"""

tf.random.set_seed(42)

TFHUB_CACHE_DIR = os.path.join(os.curdir, "my_tfhub_cache")
os.environ["TFHUB_CACHE_DIR"] = TFHUB_CACHE_DIR

import tensorflow_hub as hub

model = keras.Sequential([
    hub.KerasLayer("https://tfhub.dev/google/tf2-preview/nnlm-en-dim50/1",
                   dtype=tf.string, input_shape=[], output_shape=[50]),
    keras.layers.Dense(128, activation="relu"),
    keras.layers.Dense(1, activation="sigmoid")
])
model.compile(loss="binary_crossentropy", optimizer="adam",
              metrics=["accuracy"])

for dirpath, dirnames, filenames in os.walk(TFHUB_CACHE_DIR):
    for filename in filenames:
        print(os.path.join(dirpath, filename))

import tensorflow_datasets as tfds

datasets, info = tfds.load("imdb_reviews", as_supervised=True, with_info=True)
train_size = info.splits["train"].num_examples
batch_size = 32
train_set = datasets["train"].repeat().batch(batch_size).prefetch(1)
history = model.fit(train_set, steps_per_epoch=train_size // batch_size, epochs=5)